---
phase: 01-edge-function
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - supabase/functions/chat/rateLimit.ts
  - supabase/functions/chat/queryExecutor.ts
  - supabase/functions/chat/sqlGenerator.ts
  - supabase/functions/chat/responseGenerator.ts
  - supabase/functions/chat/pipeline.ts
autonomous: true

must_haves:
  truths:
    - "Rate limiting rejects users who exceed 20 questions per hour with a friendly message"
    - "Generated SQL is executed as the authenticated Postgres role with the user's ID set via set_config so RLS policies scope results"
    - "SQL execution has a statement timeout that kills queries exceeding the limit"
    - "NL-to-SQL conversion produces either a SQL query or an off-topic redirect via structured output"
    - "Response generation streams token-by-token text deltas via SSE"
    - "The pipeline orchestrates all steps in sequence and emits step start/complete events, text deltas, response_id, conversion_id, and done"
    - "Follow-up questions chain via previous_response_id and previous_conversion_id"
  artifacts:
    - path: "supabase/functions/chat/rateLimit.ts"
      provides: "Per-user rate limiting via Upstash Redis"
      exports: ["checkRateLimit"]
    - path: "supabase/functions/chat/queryExecutor.ts"
      provides: "RLS-enforced SQL execution with statement timeout"
      exports: ["executeWithRLS"]
    - path: "supabase/functions/chat/sqlGenerator.ts"
      provides: "NL-to-SQL conversion via OpenAI GPT-4o-mini"
      exports: ["generateSQL"]
    - path: "supabase/functions/chat/responseGenerator.ts"
      provides: "Streaming friendly response via OpenAI GPT-4o-mini"
      exports: ["generateStreamingResponse"]
    - path: "supabase/functions/chat/pipeline.ts"
      provides: "Pipeline orchestrator connecting all steps"
      exports: ["runPipeline"]
  key_links:
    - from: "supabase/functions/chat/pipeline.ts"
      to: "supabase/functions/chat/sqlGenerator.ts"
      via: "import generateSQL"
      pattern: "import.*generateSQL.*from.*sqlGenerator"
    - from: "supabase/functions/chat/pipeline.ts"
      to: "supabase/functions/chat/queryExecutor.ts"
      via: "import executeWithRLS"
      pattern: "import.*executeWithRLS.*from.*queryExecutor"
    - from: "supabase/functions/chat/pipeline.ts"
      to: "supabase/functions/chat/responseGenerator.ts"
      via: "import generateStreamingResponse"
      pattern: "import.*generateStreamingResponse.*from.*responseGenerator"
    - from: "supabase/functions/chat/pipeline.ts"
      to: "supabase/functions/chat/streamHandler.ts"
      via: "import createProgressStream, createSSEHeaders"
      pattern: "import.*createProgressStream.*from.*streamHandler"
    - from: "supabase/functions/chat/pipeline.ts"
      to: "supabase/functions/chat/security.ts"
      via: "import validateSqlQuery, sanitizeUserInput"
      pattern: "import.*validateSqlQuery.*from.*security"
    - from: "supabase/functions/chat/queryExecutor.ts"
      to: "Postgres via jsr:@db/postgres"
      via: "SET LOCAL ROLE authenticated + set_config for RLS"
      pattern: "SET LOCAL ROLE authenticated"
---

<objective>
Create the pipeline files for the `chat` edge function: rate limiting, RLS-enforced query execution, NL-to-SQL generation, streaming response generation, and the pipeline orchestrator that wires them all together.

Purpose: This is the core business logic of the chat feature. It transforms a natural language question into a safe, user-scoped SQL query, executes it, and streams a friendly response.

Output: Five files in `supabase/functions/chat/` that implement the complete NL-to-SQL-to-response pipeline.
</objective>

<execution_context>
@/Users/robsnider/.claude/get-shit-done/workflows/execute-plan.md
@/Users/robsnider/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-edge-function/01-CONTEXT.md
@.planning/phases/01-edge-function/01-RESEARCH.md
@.planning/phases/01-edge-function/01-01-SUMMARY.md

# Source patterns to reference (READ these files before implementing):
@supabase/functions/ai-insights/agent.ts
@supabase/functions/ai-insights/index.ts
@supabase/functions/_utils/isEntitledTo.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create rate limiter and query executor</name>
  <files>supabase/functions/chat/rateLimit.ts, supabase/functions/chat/queryExecutor.ts</files>
  <action>
Create `supabase/functions/chat/rateLimit.ts`:
- Import `Ratelimit` from `npm:@upstash/ratelimit` and `Redis` from `npm:@upstash/redis`.
- Create the Ratelimit instance OUTSIDE the exported function (module-level) for connection reuse across warm invocations:
  ```typescript
  const ratelimit = new Ratelimit({
    redis: Redis.fromEnv(), // Uses UPSTASH_REDIS_REST_URL + UPSTASH_REDIS_REST_TOKEN
    limiter: Ratelimit.slidingWindow(20, "1 h"),
    analytics: true,
  });
  ```
- Export `checkRateLimit(userId: string): Promise<{ allowed: boolean; resetMs?: number }>`.
  - Call `ratelimit.limit(userId)`.
  - If `!success`, return `{ allowed: false, resetMs: reset }`.
  - If `success`, return `{ allowed: true }`.
  - Wrap in try/catch: if Upstash is down, LOG the error and ALLOW the request (fail open -- better to let some requests through than block everyone).

Create `supabase/functions/chat/queryExecutor.ts`:
- Import `Client` from `jsr:@db/postgres`.
- Define `const QUERY_TIMEOUT_MS = 10_000;` (10 seconds -- generous for complex aggregations, kills runaway queries).
- Define `const MAX_ROWS = 100;` (per CONTEXT.md decision).
- Export `executeWithRLS(sql: string, userId: string): Promise<{ rows: unknown[]; truncated: boolean }>`.
  - Implementation (from research, Pattern 1):
    1. Create a new `Client(Deno.env.get("SUPABASE_DB_URL"))` and connect.
    2. Begin transaction: `await client.queryObject("BEGIN")`.
    3. Set statement timeout: `await client.queryObject(\`SET LOCAL statement_timeout = '${QUERY_TIMEOUT_MS}ms'\`)`.
    4. Switch role: `await client.queryObject("SET LOCAL ROLE authenticated")`.
    5. Set JWT claims for `auth.uid()` to work:
       - `await client.queryObject(\`SELECT set_config('request.jwt.claim.sub', '${userId}', true)\`)` -- the userId comes from a verified JWT so it is trusted, but still validate UUID format before interpolation.
       - `await client.queryObject(\`SELECT set_config('request.jwt.claims', '${JSON.stringify({ sub: userId, role: "authenticated" })}', true)\`)`
    6. Execute the generated SQL: `const result = await client.queryObject(sql)`.
    7. Commit: `await client.queryObject("COMMIT")`.
    8. Truncate if needed: if rows.length > MAX_ROWS, slice to MAX_ROWS and set truncated = true.
    9. Return `{ rows, truncated }`.
  - Error handling:
    - In catch block: `await client.queryObject("ROLLBACK").catch(() => {})` then rethrow.
    - In finally block: `await client.end()`.
  - IMPORTANT: Add a UUID format validation for userId before interpolation: `/^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i`. Throw if invalid.
  </action>
  <verify>
Both files exist and have valid TypeScript syntax:
```bash
deno check supabase/functions/chat/rateLimit.ts
deno check supabase/functions/chat/queryExecutor.ts
```
- `rateLimit.ts` exports `checkRateLimit` function
- `queryExecutor.ts` exports `executeWithRLS` function
- `queryExecutor.ts` contains `SET LOCAL ROLE authenticated` and `set_config` calls
- `queryExecutor.ts` has `BEGIN`/`COMMIT`/`ROLLBACK` transaction pattern
- `queryExecutor.ts` has UUID validation before interpolation
  </verify>
  <done>
- Rate limiter uses Upstash Redis with 20 requests/hour sliding window, fails open if Upstash is down
- Query executor wraps SQL in transaction with RLS role switching, sets both JWT claim formats, enforces 10s timeout, limits to 100 rows
- userId is validated as UUID before interpolation into SQL
  </done>
</task>

<task type="auto">
  <name>Task 2: Create SQL generator, response generator, and pipeline orchestrator</name>
  <files>supabase/functions/chat/sqlGenerator.ts, supabase/functions/chat/responseGenerator.ts, supabase/functions/chat/pipeline.ts</files>
  <action>
Create `supabase/functions/chat/sqlGenerator.ts`:
- Import `OpenAI` from `npm:openai@4.103.0` and `z` from `npm:zod`.
- Import `NL_TO_SQL_INSTRUCTIONS` from `./prompts.ts`.
- Define the model: `const NL_TO_SQL_MODEL = "gpt-4o-mini";` (per CONTEXT.md decision -- NOT the fine-tuned model).
- Define the Zod schema for structured output:
  ```typescript
  const SqlResponse = z.discriminatedUnion("offTopic", [
    z.object({ offTopic: z.literal(true), redirectMessage: z.string() }),
    z.object({ offTopic: z.literal(false), sqlQuery: z.string() }),
  ]);
  ```
- Define the JSON schema for OpenAI's `text.format` (manual schema object, not using Zod helper -- same pattern as existing ai-insights to avoid "type: string" error):
  ```typescript
  const apiSchema = {
    type: "json_schema" as const,
    name: "sql_response",
    strict: false,
    schema: {
      type: "object",
      properties: {
        offTopic: { type: "boolean" },
        redirectMessage: { type: "string" },
        sqlQuery: { type: "string" },
      },
      required: ["offTopic"],
      additionalProperties: false,
    },
  };
  ```
- Export interface `SqlGenerationResult`:
  ```typescript
  export interface SqlGenerationResult {
    offTopic: boolean;
    sql?: string;
    redirectMessage?: string;
    responseId: string;
  }
  ```
- Export `generateSQL(params: { openai: OpenAI; query: string; hashedUserId: string; previousConversionId?: string }): Promise<SqlGenerationResult>`.
  - Call `openai.responses.create` with:
    - `model: NL_TO_SQL_MODEL`
    - `instructions: NL_TO_SQL_INSTRUCTIONS`
    - `input: query` (just the raw sanitized query -- no `[user_id: ...]` since RLS handles scoping)
    - `previous_response_id: previousConversionId` (if provided, for follow-ups)
    - `text: { format: apiSchema }`
    - `temperature: 0.1` (low for consistent SQL)
    - `store: true` (for follow-up context)
    - `user: hashedUserId`
    - `max_output_tokens: 300`
  - Parse the `output_text` as JSON, validate with Zod `SqlResponse.parse()`.
  - If parsing fails, attempt one self-correction retry (same pattern as existing ai-insights `convertToSql`):
    - Call `openai.responses.create` with `previous_response_id: response.id` and repair instructions.
    - Parse the repair response.
    - If repair also fails, throw.
  - Return `SqlGenerationResult` with the appropriate fields.

- Also export a `hashUserId` helper (same as existing ai-insights):
  ```typescript
  export async function hashUserId(userId: string): Promise<string> {
    const encoder = new TextEncoder();
    const data = encoder.encode(userId);
    const hashBuffer = await crypto.subtle.digest("SHA-256", data);
    const hashArray = Array.from(new Uint8Array(hashBuffer));
    return hashArray.map((b) => b.toString(16).padStart(2, "0")).join("");
  }
  ```

Create `supabase/functions/chat/responseGenerator.ts`:
- Import `OpenAI` from `npm:openai@4.103.0`.
- Import `RESPONSE_INSTRUCTIONS` from `./prompts.ts`.
- Define `const RESPONSE_MODEL = "gpt-4o-mini";` (per CONTEXT.md -- same model for both calls).
- Export `generateStreamingResponse(params: { openai: OpenAI; query: string; rows: unknown[]; truncated: boolean; previousResponseId: string; hashedUserId: string; sendTextDelta: (delta: string) => void }): Promise<{ responseId: string }>`.
  - Include a `jsonToCsv` helper (same as existing ai-insights) to convert rows to CSV for token efficiency.
  - Build user input: `User asked: "${query}"\n\nData results:\n${csvData}${truncationNote}`.
  - Call `openai.responses.create` with:
    - `model: RESPONSE_MODEL`
    - `instructions: RESPONSE_INSTRUCTIONS`
    - `input: userInput`
    - `previous_response_id: previousResponseId` (chain from SQL generation for context)
    - `stream: true`
    - `temperature: 0.7` (natural responses)
    - `store: true`
    - `user: hashedUserId`
    - `max_output_tokens: 1000`
  - Iterate over stream events:
    - On `response.output_text.delta`: call `sendTextDelta(delta)`.
    - On `response.completed`: capture `responseId`.
  - Return `{ responseId }`.

Create `supabase/functions/chat/pipeline.ts`:
- Import all dependencies: `generateSQL`, `hashUserId` from `./sqlGenerator.ts`, `generateStreamingResponse` from `./responseGenerator.ts`, `executeWithRLS` from `./queryExecutor.ts`, `validateSqlQuery`, `sanitizeUserInput` from `./security.ts`, `createProgressStream`, `createSSEHeaders` from `./streamHandler.ts`.
- Import `OpenAI` from `npm:openai@4.103.0`.
- Export interface `PipelineInput`:
  ```typescript
  export interface PipelineInput {
    query: string;
    userId: string;
    previousResponseId?: string;
    previousConversionId?: string;
  }
  ```
- Export `runPipeline(input: PipelineInput): Promise<Response>`.
  - Implementation follows the pattern from existing ai-insights `runPipeline` but with the new event protocol:
    1. Create OpenAI client, progress stream, hash userId.
    2. Sanitize input: `const sanitizedQuery = sanitizeUserInput(input.query)`.
    3. Run pipeline asynchronously (same `(async () => { ... })()` pattern as ai-insights):

    **Step 1: "Understanding your question..."**
    - `progress.sendStep("Understanding your question...", "start")`
    - Call `generateSQL({ openai, query: sanitizedQuery, hashedUserId, previousConversionId })`.
    - `progress.sendStep("Understanding your question...", "complete")`
    - `progress.sendConversionId(result.responseId)` -- send conversion ID for follow-ups.
    - If `result.offTopic === true`: send the redirect message as text deltas (split into a single text_delta), send done, close, return.

    **SQL Validation (silent -- no step event per CONTEXT.md)**
    - Call `validateSqlQuery(result.sql)`.
    - If invalid: `progress.sendError("I generated something that doesn't look right. Could you try rephrasing?")`, close, return.

    **Step 2: "Looking up your data..."**
    - `progress.sendStep("Looking up your data...", "start")`
    - Call `executeWithRLS(result.sql, input.userId)`.
    - `progress.sendStep("Looking up your data...", "complete")`

    **Step 3: Stream response (no step event -- per CONTEXT.md, no spinner during response streaming)**
    - Call `generateStreamingResponse({ openai, query: sanitizedQuery, rows, truncated, previousResponseId: sqlResult.responseId, hashedUserId, sendTextDelta: (delta) => progress.sendTextDelta(delta) })`.
    - `progress.sendResponseId(responseResult.responseId)` -- send response ID for follow-ups.
    - `progress.sendDone()` -- completion signal.

    **Error handling:**
    - Wrap entire pipeline in try/catch. On error: `console.error`, `progress.sendError("I ran into an issue. Could you try again?")`.
    - In finally: `progress.close()`.

    4. Return `new Response(progress.stream, { headers: createSSEHeaders() })`.
  </action>
  <verify>
All three files exist and have valid TypeScript syntax:
```bash
deno check supabase/functions/chat/sqlGenerator.ts
deno check supabase/functions/chat/responseGenerator.ts
deno check supabase/functions/chat/pipeline.ts
```
- `pipeline.ts` imports from all other chat function files
- `pipeline.ts` sends exactly 2 user-visible steps ("Understanding your question...", "Looking up your data...")
- SQL validation is silent (no step event)
- Response streaming has no step event (text deltas only)
- Off-topic questions return a redirect message via text_delta events
- `sendDone()` is called on successful completion
  </verify>
  <done>
- SQL generator calls GPT-4o-mini with structured output, handles off-topic detection, includes self-correction retry
- Response generator streams token-by-token via SSE text_delta events with encouraging coach personality
- Pipeline orchestrates: sanitize input -> generate SQL -> validate SQL (silent) -> execute with RLS -> stream response
- Only 2 user-visible steps with start/complete pairs, plus streaming text
- Follow-up context passes via previousConversionId (SQL chain) and previousResponseId (response chain)
- Off-topic questions handled gracefully with friendly redirect
- All errors produce user-friendly messages
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. All five files exist in `supabase/functions/chat/`
2. `deno check` passes on all files
3. `pipeline.ts` successfully imports from all four foundation files (Plan 01) and all three files in this plan
4. The pipeline emits events in the correct order: step start -> step complete -> step start -> step complete -> text_delta(s) -> response_id -> conversion_id -> done
5. No references to the old ai-insights event types (no "in_progress", no "text" type)
</verification>

<success_criteria>
- Rate limiter uses Upstash Redis with 20/hour sliding window and fails open
- Query executor enforces RLS via role switching, sets auth.uid() via both claim formats, enforces 10s timeout, limits 100 rows
- SQL generator uses GPT-4o-mini, handles off-topic detection, includes self-correction
- Response generator streams via SSE with encouraging personality
- Pipeline orchestrates all steps with exactly 2 user-visible progress steps
- Follow-up support via previous_response_id and previous_conversion_id
</success_criteria>

<output>
After completion, create `.planning/phases/01-edge-function/01-02-SUMMARY.md`
</output>
