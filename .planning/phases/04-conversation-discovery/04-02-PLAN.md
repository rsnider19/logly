---
phase: 04-conversation-discovery
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/functions/chat/streamHandler.ts
  - supabase/functions/chat/prompts.ts
  - supabase/functions/chat/pipeline.ts
  - lib/features/chat/data/chat_suggested_prompt_repository.dart
  - lib/features/chat/presentation/providers/chat_starter_prompts_provider.dart
  - lib/features/chat/presentation/widgets/chat_empty_state.dart
autonomous: true

must_haves:
  truths:
    - "Edge function emits follow_up SSE event with 2-3 suggestions after response"
    - "Starter questions are fetched from ai_insight_suggested_prompt table"
    - "Empty chat state shows dynamic prompts from Supabase, not hardcoded"
  artifacts:
    - path: "supabase/functions/chat/streamHandler.ts"
      provides: "sendFollowUp method in ProgressStream"
      contains: "sendFollowUp"
    - path: "supabase/functions/chat/prompts.ts"
      provides: "Follow-up generation instructions"
      contains: "FOLLOW_UP"
    - path: "lib/features/chat/data/chat_suggested_prompt_repository.dart"
      provides: "Repository to fetch starter prompts"
      contains: "ChatSuggestedPromptRepository"
    - path: "lib/features/chat/presentation/providers/chat_starter_prompts_provider.dart"
      provides: "Provider for starter prompts"
      contains: "@riverpod"
    - path: "lib/features/chat/presentation/widgets/chat_empty_state.dart"
      provides: "Dynamic starter questions UI"
      contains: "chatStarterPromptsProvider"
  key_links:
    - from: "supabase/functions/chat/pipeline.ts"
      to: "streamHandler.ts"
      via: "sendFollowUp call"
      pattern: "progress\\.sendFollowUp"
    - from: "lib/features/chat/presentation/widgets/chat_empty_state.dart"
      to: "chat_starter_prompts_provider.dart"
      via: "ref.watch"
      pattern: "chatStarterPromptsProvider"
---

<objective>
Add follow-up suggestions to the edge function SSE protocol and implement dynamic starter questions in the Flutter UI.

Purpose: Enable contextual follow-up suggestions after each AI response (CHAT-09) and dynamic starter questions on empty chat (DISC-01).
Output: Edge function emits follow_up events, Flutter fetches starter prompts from Supabase.
</objective>

<execution_context>
@/Users/robsnider/.claude/get-shit-done/workflows/execute-plan.md
@/Users/robsnider/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-conversation-discovery/04-RESEARCH.md

# Edge function code
@supabase/functions/chat/streamHandler.ts
@supabase/functions/chat/pipeline.ts
@supabase/functions/chat/prompts.ts
@supabase/functions/chat/responseGenerator.ts

# Flutter code
@lib/features/chat/presentation/widgets/chat_empty_state.dart
@supabase/migrations/20251224103300_ai_insight_suggested_prompts.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add follow-up suggestions to edge function SSE protocol</name>
  <files>
supabase/functions/chat/streamHandler.ts
supabase/functions/chat/prompts.ts
supabase/functions/chat/pipeline.ts
  </files>
  <action>
**Update streamHandler.ts:**

1. Add FollowUpMessage interface after DoneMessage:
```typescript
export interface FollowUpMessage {
  type: "follow_up";
  suggestions: string[];
}
```

2. Add to StreamMessage union type:
```typescript
export type StreamMessage =
  | StepMessage
  | TextDeltaMessage
  | ResponseIdMessage
  | ConversionIdMessage
  | ErrorMessage
  | DoneMessage
  | FollowUpMessage;
```

3. Add to ProgressStream interface:
```typescript
/** Send follow-up question suggestions. */
sendFollowUp(suggestions: string[]): void;
```

4. Implement in createProgressStream return object:
```typescript
sendFollowUp(suggestions: string[]): void {
  sendMessage({ type: "follow_up", suggestions });
},
```

**Update prompts.ts:**

Add follow-up instructions to RESPONSE_INSTRUCTIONS. Append after the existing instructions:

```typescript
// Add this constant
export const FOLLOW_UP_MARKER = "<!-- FOLLOW_UPS:";

// Add to RESPONSE_INSTRUCTIONS string (at the end):
`
FOLLOW-UP SUGGESTIONS:
- At the END of your response (after all content), add a JSON block with 2-3 follow-up question suggestions
- Format exactly: <!-- FOLLOW_UPS: ["Question 1?", "Question 2?", "Question 3?"] -->
- Keep questions brief (under 40 characters each)
- Make them contextually relevant to what was just discussed
- Vary question types: comparison ("vs last week?"), detail ("which day?"), trend ("any patterns?")
- Example: <!-- FOLLOW_UPS: ["How about last month?", "What day was best?", "Any patterns?"] -->
`
```

**Update pipeline.ts:**

1. Import FOLLOW_UP_MARKER from prompts.ts
2. After generateStreamingResponse completes (after sending responseId, before sendDone), extract and emit follow-ups:

```typescript
// After progress.sendResponseId(responseId);

// Extract follow-up suggestions from accumulated response text
const followUps = extractFollowUpSuggestions(fullResponseText);
if (followUps.length > 0) {
  progress.sendFollowUp(followUps);
}

progress.sendDone();
```

3. Add helper function at bottom of file:
```typescript
/**
 * Extracts follow-up suggestions from the response text.
 * Looks for: <!-- FOLLOW_UPS: ["...", "..."] -->
 */
function extractFollowUpSuggestions(text: string): string[] {
  const marker = "<!-- FOLLOW_UPS:";
  const startIdx = text.indexOf(marker);
  if (startIdx === -1) return [];

  const endIdx = text.indexOf("-->", startIdx);
  if (endIdx === -1) return [];

  const jsonStr = text.slice(startIdx + marker.length, endIdx).trim();
  try {
    const parsed = JSON.parse(jsonStr);
    if (Array.isArray(parsed) && parsed.every(s => typeof s === "string")) {
      return parsed.slice(0, 3); // Max 3 suggestions
    }
  } catch {
    console.warn("[Pipeline] Failed to parse follow-up suggestions:", jsonStr);
  }
  return [];
}
```

4. Track fullResponseText by accumulating text deltas. Modify the onTextDelta callback:
```typescript
let fullResponseText = "";
const { responseId } = await generateStreamingResponse({
  // ... existing params
  onTextDelta: (delta) => {
    fullResponseText += delta;
    // Strip follow-up marker from streamed text so it doesn't appear in UI
    const cleanDelta = delta.includes("<!-- FOLLOW_UPS:")
      ? delta.split("<!-- FOLLOW_UPS:")[0]
      : delta;
    if (cleanDelta) progress.sendTextDelta(cleanDelta);
  },
});
```

IMPORTANT: The follow-up marker must NOT appear in the streamed text to the user. Filter it out during streaming.
  </action>
  <verify>Deploy edge function locally with `supabase functions serve chat --env-file ./supabase/.env.local` and test with curl - response should include follow_up event before done event</verify>
  <done>Edge function emits follow_up SSE event with array of suggestion strings</done>
</task>

<task type="auto">
  <name>Task 2: Create starter prompts repository and provider</name>
  <files>
lib/features/chat/data/chat_suggested_prompt_repository.dart
lib/features/chat/presentation/providers/chat_starter_prompts_provider.dart
  </files>
  <action>
**Create lib/features/chat/data/chat_suggested_prompt_repository.dart:**

```dart
import 'package:logly/services/supabase_service.dart';
import 'package:riverpod_annotation/riverpod_annotation.dart';

part 'chat_suggested_prompt_repository.g.dart';

/// Repository for fetching starter question prompts from Supabase.
///
/// Reads from the existing `ai_insight_suggested_prompt` table.
class ChatSuggestedPromptRepository {
  ChatSuggestedPromptRepository(this._supabase);

  final SupabaseService _supabase;

  /// Fetches active starter prompts ordered by display_order.
  ///
  /// Returns a list of prompt strings to display in the empty chat state.
  Future<List<String>> getActivePrompts() async {
    final response = await _supabase.client
        .from('ai_insight_suggested_prompt')
        .select('prompt_text')
        .eq('is_active', true)
        .order('display_order');

    return (response as List<dynamic>)
        .map((row) => row['prompt_text'] as String)
        .toList();
  }
}

@Riverpod(keepAlive: true)
ChatSuggestedPromptRepository chatSuggestedPromptRepository(
  ChatSuggestedPromptRepositoryRef ref,
) {
  return ChatSuggestedPromptRepository(ref.watch(supabaseServiceProvider));
}
```

**Create lib/features/chat/presentation/providers/chat_starter_prompts_provider.dart:**

```dart
import 'package:logly/features/chat/data/chat_suggested_prompt_repository.dart';
import 'package:riverpod_annotation/riverpod_annotation.dart';

part 'chat_starter_prompts_provider.g.dart';

/// Provider for starter prompts shown in the empty chat state.
///
/// Fetches from Supabase on first access and caches the result.
/// Falls back to hardcoded prompts if the fetch fails.
@riverpod
Future<List<String>> chatStarterPrompts(ChatStarterPromptsRef ref) async {
  final repository = ref.watch(chatSuggestedPromptRepositoryProvider);

  try {
    final prompts = await repository.getActivePrompts();
    if (prompts.isNotEmpty) return prompts;
  } catch (e) {
    // Log error but don't crash - fall back to defaults
  }

  // Fallback prompts if fetch fails or returns empty
  return const [
    'What did I do this week?',
    'What are my most consistent habits?',
    'How active was I last month?',
  ];
}
```

Run build_runner after creating these files.
  </action>
  <verify>Run `fvm dart run build_runner build --delete-conflicting-outputs` - should generate .g.dart files for both</verify>
  <done>Repository fetches from ai_insight_suggested_prompt, provider caches result with fallback</done>
</task>

<task type="auto">
  <name>Task 3: Update ChatEmptyState to use dynamic prompts</name>
  <files>lib/features/chat/presentation/widgets/chat_empty_state.dart</files>
  <action>
Convert ChatEmptyState from StatelessWidget to ConsumerWidget to access Riverpod providers.

**Update lib/features/chat/presentation/widgets/chat_empty_state.dart:**

1. Import the provider:
```dart
import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'package:logly/features/chat/presentation/providers/chat_starter_prompts_provider.dart';
```

2. Change class declaration:
```dart
class ChatEmptyState extends ConsumerWidget {
  const ChatEmptyState({required this.onSuggestionTap, super.key});

  final void Function(String question) onSuggestionTap;

  @override
  Widget build(BuildContext context, WidgetRef ref) {
```

3. Watch the provider and handle loading/error states:
```dart
final promptsAsync = ref.watch(chatStarterPromptsProvider);
```

4. In the Wrap children, replace hardcoded chips with:
```dart
...promptsAsync.when(
  data: (prompts) => prompts.map((prompt) => _SuggestionChip(
    label: prompt,
    onTap: () => onSuggestionTap(prompt),
  )).toList(),
  loading: () => [
    // Show shimmer placeholder chips during loading
    for (int i = 0; i < 3; i++)
      const _ShimmerChip(),
  ],
  error: (_, __) => [
    // Fallback to static prompts on error (provider already has fallback, but defensive)
    _SuggestionChip(
      label: 'What did I do this week?',
      onTap: () => onSuggestionTap('What did I do this week?'),
    ),
  ],
),
```

5. Add shimmer placeholder widget for loading state:
```dart
class _ShimmerChip extends StatelessWidget {
  const _ShimmerChip();

  @override
  Widget build(BuildContext context) {
    final theme = Theme.of(context);
    return Container(
      width: 180,
      height: 36,
      decoration: BoxDecoration(
        color: theme.colorScheme.surfaceContainerHighest.withOpacity(0.5),
        borderRadius: BorderRadius.circular(20),
      ),
    );
  }
}
```

The shimmer chips keep the same visual shape as real chips, providing a smooth loading experience.
  </action>
  <verify>Run the app and navigate to chat screen with no messages - should show prompts from Supabase (or fallback if network issue)</verify>
  <done>ChatEmptyState fetches dynamic prompts from Supabase, shows shimmer during loading, falls back to hardcoded on error</done>
</task>

</tasks>

<verification>
1. Edge function has follow_up event: grep "sendFollowUp" supabase/functions/chat/streamHandler.ts
2. Pipeline extracts follow-ups: grep "extractFollowUpSuggestions" supabase/functions/chat/pipeline.ts
3. Repository exists: ls lib/features/chat/data/chat_suggested_prompt_repository.dart
4. Provider exists: ls lib/features/chat/presentation/providers/chat_starter_prompts_provider.dart
5. ChatEmptyState uses provider: grep "chatStarterPromptsProvider" lib/features/chat/presentation/widgets/chat_empty_state.dart
6. Codegen passes: `fvm dart run build_runner build --delete-conflicting-outputs` exits 0
7. No analyzer errors: `fvm flutter analyze`
</verification>

<success_criteria>
- Edge function SSE protocol includes follow_up event type
- Follow-up suggestions extracted from response text (marker stripped from UI)
- Starter prompts fetched from ai_insight_suggested_prompt table
- ChatEmptyState shows dynamic prompts with loading shimmer
- Fallback prompts work when Supabase fetch fails
- Zero analyzer errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-conversation-discovery/04-02-SUMMARY.md`
</output>
